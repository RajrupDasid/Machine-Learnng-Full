{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d5c933-809e-4faa-8b61-2d5abb5fc526",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81764191-ec4c-4210-8e53-e4c4a66943d6",
   "metadata": {},
   "source": [
    "##### Improve performance with extra capacity or early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db6704-e6e1-47cc-b0b3-911a67794f0c",
   "metadata": {},
   "source": [
    "Recall from the example in the previous lesson that Keras will keep a history of the training and validation loss over the epochs that it is training the model. In this lesson, we're going to learn how to interpret these learning curves and how we can use them to guide model development. In particular, we'll examine at the learning curves for evidence of underfitting and overfitting and look at a couple of strategies for correcting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358037af-768f-4fc0-b8a9-665d7648dc5f",
   "metadata": {},
   "source": [
    "# Interpreting the Learning CurvesÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1acd9b-5528-4796-9443-aa4fddb66439",
   "metadata": {},
   "source": [
    "You might think about the information in the training data as being of two kinds: signal and noise. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. The noise is the part might look useful but really isn't.\n",
    "\n",
    "We train a model by choosing weights or parameters that minimize the loss on a training set. You might know, however, that to accurately assess a model's performance, we need to evaluate it on a new set of data, the validation data. (You could see our lesson on model validation in Introduction to Machine Learning for a review.)\n",
    "\n",
    "\n",
    "When we train a model we've been plotting the loss on the training set epoch by epoch. To this we'll add a plot the validation data too. These plots we call the learning curves. To train deep learning models effectively, we need to be able to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb201a1-8d69-4dec-9661-573ed1b294a7",
   "metadata": {},
   "source": [
    "<img src=\"./refimages/uau.png\" >\n",
    "\n",
    "The validation loss gives an estimate of the expected error on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51e195-32e7-4b0a-88ae-26ad546ec133",
   "metadata": {},
   "source": [
    "Now, the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. (Whatever noise the model learned from the training set won't generalize to new data.) So, when a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c6b70-2491-40e4-8654-2267447d998b",
   "metadata": {},
   "source": [
    "Ideally, we would create models that learn all of the signal and none of the noise. This will practically never happen. Instead we make a trade. We can get the model to learn more signal at the cost of learning more noise. So long as the trade is in our favor, the validation loss will continue to decrease. After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97523aea-89c7-42e4-93b1-306302ed8f01",
   "metadata": {},
   "source": [
    "<img src=\"./refimages/uo.png\">\n",
    "\n",
    "Underfitting and overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e45a53-09e4-43ab-a67e-18c00625986d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
